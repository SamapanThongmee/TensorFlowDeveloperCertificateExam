{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
    "# Please note that the weight of the grade for the question is relative\n",
    "# to its difficulty. So your Category 1 question will score significantly\n",
    "# less than your Category 5 question.\n",
    "#\n",
    "# Don't use lambda layers in your model.\n",
    "# You do not need them to solve the question.\n",
    "# Lambda layers are not supported by the grading infrastructure.\n",
    "#\n",
    "# You must use the Submit and Test button to submit your model\n",
    "# at least once in this category before you finally submit your exam,\n",
    "# otherwise you will score zero for this category.\n",
    "# ======================================================================\n",
    "#\n",
    "# NLP QUESTION\n",
    "#\n",
    "# Build and train a classifier for the sarcasm dataset.\n",
    "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
    "# It will be tested against a number of sentences that the network hasn't previously seen\n",
    "# and you will be scored on whether sarcasm was correctly detected in those sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install json\n",
    "%pip install tensorflow\n",
    "%pip install numpy\n",
    "%pip install urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def solution_model():\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(\"./sarcasm.json\", 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "\n",
    "# Collect sentences and labels into the lists\n",
    "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_length = 120\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_size = 20000\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    # YOUR CODE HERE\n",
    "    for item in datastore:\n",
    "      sentences.append(item['headline'])\n",
    "      labels.append(item['is_sarcastic'])\n",
    "\n",
    "    # Split the sentences\n",
    "    training_sentences = sentences[:training_size]\n",
    "    testing_sentences = sentences[training_size:]\n",
    "\n",
    "    # Split the labels\n",
    "    training_labels = labels[:training_size]\n",
    "    testing_labels = labels[training_size:]\n",
    "\n",
    "    # Initialize the Tokenizer class\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "    # Generate the word index dictionary\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Generate and pad the training sequences\n",
    "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    # Generate and pad the testing sequences\n",
    "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    # Convert the labels lists into numpy arrays\n",
    "    training_labels = np.array(training_labels)\n",
    "    testing_labels = np.array(testing_labels)\n",
    "\n",
    "    lstm_dim = 32\n",
    "    dense_dim = 24\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim)),\n",
    "        tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Accuracy 95%\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if logs.get('accuracy') is not None and logs.get('accuracy') > 0.95:\n",
    "                print(\"\\nReached 99.5% accuracy so cancelling training!\")\n",
    "                self.model.stop_training = True\n",
    "    callbacks = myCallback()\n",
    "\n",
    "    # Checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"Question_4.h5\",\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "        )\n",
    "\n",
    "    model.fit(training_padded, training_labels,\n",
    "              validation_data=(testing_padded, testing_labels),\n",
    "              epochs=500,\n",
    "              callbacks=[callbacks, checkpoint])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you'll need to save your model as a .h5 like this.\n",
    "# When you press the Submit and Test button, your saved .h5 model will\n",
    "# be sent to the testing infrastructure for scoring\n",
    "# and the score will be returned to you.\n",
    "if __name__ == '__main__':\n",
    "    model = solution_model()\n",
    "    model.save(\"Question_4.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
